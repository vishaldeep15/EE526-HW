\documentclass[letterpaper , 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{listings} % code formatting
\usepackage{enumitem} % change enumeration counter
\usepackage{amsmath}
\usepackage{graphicx}

%\usepackage{pythonhighlight}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pythonhighlight}

\lstset{
	basicstyle=\ttfamily,
	columns=fullflexible,
	frame=single,
	breaklines=true,
}

\begin{document}
	
\begin{titlepage}
	
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
	
	\center % Center everything on the page
	
	%----------------------------------------------------------------------------------------
	%	HEADING SECTIONS
	%----------------------------------------------------------------------------------------
	
	\textsc{\LARGE Iowa State University}\\[1.5cm] % Name of your university/college
	\textsc{\LARGE Department of Electrical and Computer Engineering}\\[1.0cm]
	\textsc{\Large Deep Machine Learning: Theory and Practice}\\[0.5cm] % Major heading such as course name
	\textsc{\large EE 526X}\\[0.5cm] % Minor heading such as course title
	
	%----------------------------------------------------------------------------------------
	%	TITLE SECTION
	%----------------------------------------------------------------------------------------
	
	\HRule \\[0.4cm]
	{ \huge \bfseries Final Project Report}\\[0.4cm] % Title of your document
	\HRule \\[1.5cm]
	
	%----------------------------------------------------------------------------------------
	%	AUTHOR SECTION
	%----------------------------------------------------------------------------------------
	
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
			\emph{Author:}\\
			Vishal \textsc{Deep} % Your name
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright} \large
			\emph{Instructor:} \\
			Dr. Zhengdao \textsc{Wang} % Supervisor's Name
		\end{flushright}
	\end{minipage}\\[2cm]
	
	% If you don't want a supervisor, uncomment the two lines below and remove the section above
	%\Large \emph{Author:}\\
	%John \textsc{Smith}\\[3cm] % Your name
	
	%----------------------------------------------------------------------------------------
	%	DATE SECTION
	%----------------------------------------------------------------------------------------
	
	{\large \today}\\[5cm] % Date, change the \today to a set date if you want to be precise
	
	%----------------------------------------------------------------------------------------
	%	LOGO SECTION
	%----------------------------------------------------------------------------------------
	
	\includegraphics[scale=0.3]{isu.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
	
	%----------------------------------------------------------------------------------------
	
	\vfill % Fill the rest of the page with whitespace
	
\end{titlepage}
%Header-Make sure you update this information!!!!
%\noindent
%\large\textbf{Homework21} \hfill \textbf{Vishal Deep} \\
%\normalsize CPRE 581  \hfill ID: 010639648 \\
%Dr. Zhengdao Duwe \hfill Due Date: 09/05/18 \\


\section{Problem Statement}
The main goal of the project is to design and optimize a reinforcement learning algorithm using double Q-learning on Acrobot-v1 environment from Open AI gym package \cite{openAI-gym}. The function approximation should be done with multi-layer neural networks. 

\subsection*{Acrobot-v1}
The Acrobot-v1 environment consists of two joints. One joint is fixed and other joint is actuated as shown in \cref{fig:acrobot}. In the initial stage the joints are hanging downwards and the goal is to make lower joint reach a certain given height. This environment is unsolved environment which means it does not have a specific reward above which we can say that it is solved.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{acrobot.jpg}
	\caption{Acrobot-v1 from Open AI gym package}
	\label{fig:acrobot}
\end{figure}

\begin{itemize}
	\item \textbf{State}: In this environment states consists of $\sin$ and $\cos$ of the two rotational joint angles and their angular velocities. A complete state is \newline
	[cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2].
	\item \textbf{Action}: There are three possible actions in this environment. The action is either applying +1, 0 or -1 torque on the joint between the two links.
\end{itemize}

\subsection*{Deep Q-learning}
In the Q-learning algorithm, a virtual table of action and Q-values is used. This table could grow really fast depending on the states and actions. Therefore a better solution is to create Neural Networks (NNs) which can approximate the Q-values for every action. This is called deep Q-learning. In case of Q-learning, taking the maximum overestimated values introduces a maximization bias in learning.  To solve this problem, we use a double Q-learning algorithm which uses two separate Q-value estimators. These estimators are used to update each other and thereby providing unbiased estimation of Q-values. In this project, I will be using double Q-learning function estimators using neural networks.

\section{Neural Network Model}
In this project, I have used the neural network shown in \cref{fig:nn}. This NN has three fully connected dense layers. The first layer consists of 48 neurons, the second one has 24 neurons, and last layer consists of 3 neurons to signify the 3 output actions. The total number of parameters are 1587.  

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{model.png}
	\caption{Neural Network Model used for function approximation in double Q-learning algorithm}
	\label{fig:nn}
\end{figure}

\section{Pseudo-code for the Algorithm}
%\begin{algorithm}[H]
%	Initialize primary $Q_{A}$ and target network $Q_{B}$
%	\For{\texttt{each episode}} \Do
%		\For{each step} \Do
%			Choose action $a_t$ which maximize q-value
%			Execute $a_t$ and observe State $S_{t+1}$ and Reward $R_t$
%			Save ($S_{t}, $a_t$, $R_t$, $S_{t+1}$) to memory
%			
%	\EndFor
%	\caption{Double Q-learning Algorithm}
%	\label{algo:doubleQ}
%\end{algorithm}

\begin{algorithm}[H]
	\caption{Double Q-learning Algorithm \cite{wang2015dueling}}
	\label{algo:doubleQ}
	\begin{algorithmic}[1]
		\State Initialize primary $Q_{A}$ and target network $Q_{B}$
		\For{each episode} 
			\For{each step}
				\State Choose action $a_t$ which maximize q-value
				\State Execute $a_t$ and observe State $S_{t+1}$ and Reward $R_t$
				\State Save ($S_{t}$, $a_t$, $R_t$, $S_{t+1}$) to memory
				
				\For{samples in memory}
					\State Train primary Neural Network $Q_{A}$ using samples
					\State Compute Target Q-value
					\State $Q(s_{t}, a{t}) \approx R_{t} + \gamma Q(s_{t+1}, argmax_{a^{'}} Q^{'}(s_{t}, a_{t}))$
				\EndFor
				
				\State Update target network $Q_{B}$ at each update interval:
				\State $\theta^{'} \leftarrow \tau * \theta + (1-\tau) * \theta^{'} $
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Python Code}
The code is inspired from several tutorials on the internet including \cite{ankit:handson, yash:RL}


\inputpython{../acrobot-v1.py}{1}{200}

\begin{lstlisting}[language=Python, basicstyle=\tiny]
episode: 0, score: 225, epsilon: 0.265721, Rewards: -274.0
episode: 1, score: 231, epsilon: 0.141215, Rewards: -268.0
episode: 2, score: 356, epsilon: 0.102946, Rewards: -143.0
episode: 3, score: 301, epsilon: 0.0675426, Rewards: -198.0
episode: 4, score: 328, epsilon: 0.0443147, Rewards: -171.0
episode: 5, score: 353, epsilon: 0.0323054, Rewards: -146.0
episode: 6, score: 284, epsilon: 0.019076, Rewards: -215.0
episode: 7, score: 283, epsilon: 0.0112642, Rewards: -216.0
episode: 8, score: 318, epsilon: 0.00739044, Rewards: -181.0
episode: 9, score: 340, epsilon: 0.00484887, Rewards: -159.0
episode: 10, score: 341, epsilon: 0.00318134, Rewards: -158.0
episode: 11, score: 321, epsilon: 0.00208728, Rewards: -178.0
episode: 12, score: 344, epsilon: 0.00136946, Rewards: -155.0
episode: 13, score: 302, epsilon: 0.001, Rewards: -197.0
episode: 14, score: 318, epsilon: 0.001, Rewards: -181.0
episode: 15, score: 220, epsilon: 0.001, Rewards: -279.0
episode: 16, score: 247, epsilon: 0.001, Rewards: -252.0
episode: 17, score: 318, epsilon: 0.001, Rewards: -181.0
episode: 18, score: 208, epsilon: 0.001, Rewards: -291.0
episode: 19, score: 198, epsilon: 0.001, Rewards: -301.0
episode: 20, score: 192, epsilon: 0.001, Rewards: -307.0
\end{lstlisting}

\noindent Training time for 100 episodes was 54.81 minutes \newline
Training time for 20 episodes was 19.26 minutes

\section{Lessons Learned}
\begin{enumerate}
	\item \textbf{Hyper-parameter tuning}: The hyper-parameters play a really important part in helping the NN function estimators to learn and predict the Q-values. In this project, I had to tune these parameters manually to see the effect in overall performance. This manual tuning is difficult and needs a lot of effort to get good results
	\item \textbf{Double Q-learning}: Double Q-learning performs much better than Q-learning algorithm and helps to converge faster. Working on this project helped me understand how double Q-learning algorithm works in practice and how it can be applied to standard benchmarks like Open AI gym package.
	\item \textbf{Exploitation vs Exploration}: The exploration and exploitation plays a big role in making algorithm converge to a good predicted values. In this project, the exploration is done in first few episodes and then the algorithm starts doing exploitation. This was controlled by hyper-parameter $\epsilon$, which was dynamically changed to get benefits from this concept.
\end{enumerate}

\section{Possible Improvements}
\begin{enumerate}
	\item \textbf{Prioritized Experience Replay (PER) \cite{schaul2015prioritized}}: This is based on the concept that some experiences are more important for the training but they don't occur frequently because we are choosing samples from the memory randomly. In PER, sampling distribution is changed by having tuples of experience with priorities.
	\item \textbf{Dueling DQN} \cite{wang2015dueling}: In this we separate the NN function estimators into two elements: one that estimates the state value and other that estimates the advantage for each action. The advantage of DDQN is that it can learn the valuable states without having to learn the actions each state.
\end{enumerate}


\bibliography{references} 
\bibliographystyle{ieeetr}

\end{document}
